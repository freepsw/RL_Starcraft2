{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8V2E13xR5nX"
   },
   "source": [
    "# How to train StarCraft II Bots\n",
    "\n",
    "In this notebook, we will get started with StarCraft II Machine Learning on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwYYNIbITWgr"
   },
   "source": [
    "# Prerequisite steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLOlLbH2R7Aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.7/site-packages (20.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below to use dev branch of pysc2\n",
    "#!pip install git+https://github.com/deepmind/pysc2.git@dev\n",
    "\n",
    "# Note: Colab does not have an X Server, installing a virtual one\n",
    "!pip install --upgrade pip\n",
    "!sudo pip install -q pysc2 pyvirtualdisplay\n",
    "# !sudo apt-get -y update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\n",
      "E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y xvfb python-opengl mesa-utils libosmesa6-dev xorg x11-xserver-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.7/site-packages (20.2.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: pysc2 in /opt/conda/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.15.0)\n",
      "Requirement already satisfied: deepdiff in /opt/conda/lib/python3.7/site-packages (from pysc2) (5.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pysc2) (2.24.0)\n",
      "Requirement already satisfied: s2clientprotocol>=4.10.1.75800.0 in /opt/conda/lib/python3.7/site-packages (from pysc2) (5.0.2.81102.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pysc2) (0.18.2)\n",
      "Requirement already satisfied: pygame in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.9.6)\n",
      "Requirement already satisfied: absl-py>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from pysc2) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.18.5)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.7/site-packages (from pysc2) (0.57.0)\n",
      "Requirement already satisfied: whichcraft in /opt/conda/lib/python3.7/site-packages (from pysc2) (0.6.1)\n",
      "Requirement already satisfied: portpicker>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.3.1)\n",
      "Requirement already satisfied: sk-video in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.1.10)\n",
      "Requirement already satisfied: protobuf>=2.6 in /opt/conda/lib/python3.7/site-packages (from pysc2) (3.12.4)\n",
      "Requirement already satisfied: enum34 in /opt/conda/lib/python3.7/site-packages (from pysc2) (1.1.10)\n",
      "Requirement already satisfied: mpyq in /opt/conda/lib/python3.7/site-packages (from pysc2) (0.2.5)\n",
      "Requirement already satisfied: s2protocol in /opt/conda/lib/python3.7/site-packages (from pysc2) (5.0.3.81433.0)\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.7/site-packages (from pysc2) (4.0.2)\n",
      "Requirement already satisfied: ordered-set>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from deepdiff->pysc2) (4.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pysc2) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pysc2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pysc2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pysc2) (1.25.10)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sk-video->pysc2) (1.5.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=2.6->pysc2) (49.6.0.post20200814)\n",
      "Requirement already satisfied: tensorboard==1.14.0 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (0.10.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (3.12.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (1.31.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (1.18.5)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (0.35.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==1.14.0) (49.6.0.post20200814)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==1.14.0) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==1.14.0) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install pysc2\n",
    "!pip install tensorboard==1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5NEmiwgSLR5"
   },
   "source": [
    "## Download StarCraft II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZX0JoA3dZB2h"
   },
   "source": [
    "Note: By typing in the password ‘iagreetotheeula’ you agree to be bound by the terms of Blizzard's [AI and Machine Learning License](http://blzdistsc2-a.akamaihd.net/AI_AND_MACHINE_LEARNING_LICENSE.html)\n",
    "\n",
    "Blizzard's CDNs are not very fast, so if you have space free in your Google Drive, I highly recommend uploading StarCraft II onto Google Drive and download from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9viW3ESbSNhy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-19 06:46:38--  https://blzdistsc2-a.akamaihd.net/Linux/SC2.4.0.2.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.16, 23.46.149.25\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3457493499 (3.2G) [application/zip]\n",
      "Saving to: ‘SC2.4.0.2.zip’\n",
      "\n",
      "SC2.4.0.2.zip       100%[===================>]   3.22G   141MB/s    in 23s     \n",
      "\n",
      "2020-09-19 06:47:01 (143 MB/s) - ‘SC2.4.0.2.zip’ saved [3457493499/3457493499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://blzdistsc2-a.akamaihd.net/Linux/SC2.4.0.2.zip\n",
    "!unzip -P iagreetotheeula -oq SC2.4.0.2.zip -d ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvMu0KmvSF8j"
   },
   "source": [
    "## Getting the maps\n",
    "\n",
    "Like StarCraft II itself, I recommend downloading all the maps and uploading it to Google Drive for it to download faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRk95lbLSFL9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-19 06:48:35--  https://github.com/deepmind/pysc2/releases/download/v1.0/mini_games.zip\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/98336690/2319096a-7d30-11e7-92cd-8b4177b0e5a7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200919T064835Z&X-Amz-Expires=300&X-Amz-Signature=5793b7b62f3b19438d88dae4c308ad285560abeac6cec29ccd84ab0f0bc70323&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=98336690&response-content-disposition=attachment%3B%20filename%3Dmini_games.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-09-19 06:48:35--  https://github-production-release-asset-2e65be.s3.amazonaws.com/98336690/2319096a-7d30-11e7-92cd-8b4177b0e5a7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200919T064835Z&X-Amz-Expires=300&X-Amz-Signature=5793b7b62f3b19438d88dae4c308ad285560abeac6cec29ccd84ab0f0bc70323&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=98336690&response-content-disposition=attachment%3B%20filename%3Dmini_games.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 54.231.114.219\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|54.231.114.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 220786 (216K) [application/octet-stream]\n",
      "Saving to: ‘mini_games.zip’\n",
      "\n",
      "mini_games.zip      100%[===================>] 215.61K   893KB/s    in 0.2s    \n",
      "\n",
      "2020-09-19 06:48:36 (893 KB/s) - ‘mini_games.zip’ saved [220786/220786]\n",
      "\n",
      "--2020-09-19 06:48:36--  https://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2017Season1.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.25, 23.46.149.16\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32710037 (31M) [application/zip]\n",
      "Saving to: ‘Ladder2017Season1.zip’\n",
      "\n",
      "Ladder2017Season1.z 100%[===================>]  31.19M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-09-19 06:48:36 (223 MB/s) - ‘Ladder2017Season1.zip’ saved [32710037/32710037]\n",
      "\n",
      "--2020-09-19 06:48:37--  https://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2017Season2.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.25, 23.46.149.16\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33631864 (32M) [application/zip]\n",
      "Saving to: ‘Ladder2017Season2.zip’\n",
      "\n",
      "Ladder2017Season2.z 100%[===================>]  32.07M   134MB/s    in 0.2s    \n",
      "\n",
      "2020-09-19 06:48:38 (134 MB/s) - ‘Ladder2017Season2.zip’ saved [33631864/33631864]\n",
      "\n",
      "--2020-09-19 06:48:38--  https://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2017Season3_Updated.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.16, 23.46.149.25\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24575335 (23M) [application/zip]\n",
      "Saving to: ‘Ladder2017Season3_Updated.zip’\n",
      "\n",
      "Ladder2017Season3_U 100%[===================>]  23.44M   125MB/s    in 0.2s    \n",
      "\n",
      "2020-09-19 06:48:39 (125 MB/s) - ‘Ladder2017Season3_Updated.zip’ saved [24575335/24575335]\n",
      "\n",
      "--2020-09-19 06:48:40--  https://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2017Season4.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.25, 23.46.149.16\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22794113 (22M) [application/zip]\n",
      "Saving to: ‘Ladder2017Season4.zip’\n",
      "\n",
      "Ladder2017Season4.z 100%[===================>]  21.74M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-09-19 06:48:40 (200 MB/s) - ‘Ladder2017Season4.zip’ saved [22794113/22794113]\n",
      "\n",
      "--2020-09-19 06:48:40--  https://blzdistsc2-a.akamaihd.net/MapPacks/Ladder2018Season1.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.25, 23.46.149.16\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20300368 (19M) [application/zip]\n",
      "Saving to: ‘Ladder2018Season1.zip’\n",
      "\n",
      "Ladder2018Season1.z 100%[===================>]  19.36M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-09-19 06:48:41 (204 MB/s) - ‘Ladder2018Season1.zip’ saved [20300368/20300368]\n",
      "\n",
      "--2020-09-19 06:48:41--  https://blzdistsc2-a.akamaihd.net/MapPacks/Melee.zip\n",
      "Resolving blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)... 23.46.149.25, 23.46.149.16\n",
      "Connecting to blzdistsc2-a.akamaihd.net (blzdistsc2-a.akamaihd.net)|23.46.149.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 235477 (230K) [application/zip]\n",
      "Saving to: ‘Melee.zip’\n",
      "\n",
      "Melee.zip           100%[===================>] 229.96K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-09-19 06:48:41 (10.3 MB/s) - ‘Melee.zip’ saved [235477/235477]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir -p ~/StarCraftII/Maps/\n",
    "!wget https://github.com/deepmind/pysc2/releases/download/v1.0/mini_games.zip\n",
    "!unzip -P iagreetotheeula -oq mini_games.zip -d ~/StarCraftII/Maps/\n",
    "\n",
    "map_packs = [\"Ladder2017Season1.zip\", \"Ladder2017Season2.zip\", \"Ladder2017Season3_Updated.zip\", \"Ladder2017Season4.zip\", \"Ladder2018Season1.zip\", \"Melee.zip\"]\n",
    "\n",
    "for file in map_packs:\n",
    "    !wget https://blzdistsc2-a.akamaihd.net/MapPacks/{file}\n",
    "    !unzip -P iagreetotheeula -oq {file} -d ~/StarCraftII/Maps/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckLzQaqYrHO8"
   },
   "source": [
    "## Remove TCMalloc\n",
    "\n",
    "This is the main roadblock stopping us originally from using Google's free GPUS\n",
    "\n",
    "Note that you will get a lot of errors looking like this\n",
    "\n",
    "\n",
    "```\n",
    "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
    "```\n",
    "This is normal, since we had to get rid of TCMalloc to run StarCraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ucy1pZnzjYg7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'libtcmalloc-minimal4' for glob 'libtcmalloc*'\n",
      "Package 'libtcmalloc-minimal4' is not installed, so not removed\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Important - remove libtcmalloc\n",
    "!sudo apt-get remove libtcmalloc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0YGj3m3W3qQ"
   },
   "source": [
    "# Mount your google drive to save your model's learned parameters\n",
    "- Google Colab에서만 지원함. \n",
    "- 직접 local환경에 jupyter notebook을 설치한 경우 SDK_COFIG오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMsCgMFOZJJ9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkwqHBCTW6TA"
   },
   "source": [
    "# Load Tensorboard for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-auth==1.6.3\n",
      "  Downloading google_auth-1.6.3-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.6.3) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.6.3) (0.2.7)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.6.3) (4.1.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.6.3) (4.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth==1.6.3) (0.4.8)\n",
      "Installing collected packages: google-auth\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.20.1\n",
      "    Uninstalling google-auth-1.20.1:\n",
      "      Successfully uninstalled google-auth-1.20.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "google-cloud-storage 1.30.0 requires google-auth<2.0dev,>=1.11.0, but you'll have google-auth 1.6.3 which is incompatible.\n",
      "google-api-python-client 1.9.1 requires google-api-core<2dev,>=1.17.0, but you'll have google-api-core 1.16.0 which is incompatible.\n",
      "google-api-python-client 1.9.1 requires google-auth>=1.16.0, but you'll have google-auth 1.6.3 which is incompatible.\u001b[0m\n",
      "Successfully installed google-auth-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install google-auth==1.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyYldjbLXXSl"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/tensorboard_log\n",
    "!unzip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR_BrGryTM5q"
   },
   "source": [
    "# Now we can start Machine Learning\n",
    "\n",
    "This is an example applying NaiveDQN to a PySC2 Agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ_qv6HUBz1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#from skdrl.pytorch.model.mlp import NaiveMultiLayerPerceptron\n",
    "#from skdrl.common.memory.memory import ExperienceReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2QrwG8jpDTW"
   },
   "outputs": [],
   "source": [
    "DATA_FILE_QNET = '5_rlagent_with_vanilla_dqn_qnet'\n",
    "DATA_FILE_QNET_TARGET = '5_rlagent_with_vanilla_dqn_qnet_target'\n",
    "\n",
    "SCORE_FILE = '5_rlagent_with_vanilla_dqn_score'\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EverEUVZHX7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveMultiLayerPerceptron(\n",
      "  (hidden_act_func): ReLU()\n",
      "  (out_act_func): Identity()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (5): Identity()\n",
      "  )\n",
      ")\n",
      "tensor([[-0.2669],\n",
      "        [-0.1236],\n",
      "        [-0.0342],\n",
      "        [-0.0529],\n",
      "        [-0.1356],\n",
      "        [-0.4471],\n",
      "        [-0.2400],\n",
      "        [-0.1967],\n",
      "        [-0.1494],\n",
      "        [-0.1274],\n",
      "        [ 0.0313],\n",
      "        [-0.0789]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NaiveMultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_neurons: list = [64, 32],\n",
    "                 hidden_act_func: str = 'ReLU',\n",
    "                 out_act_func: str = 'Identity'):\n",
    "        super(NaiveMultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.hidden_act_func = getattr(nn, hidden_act_func)()\n",
    "        self.out_act_func = getattr(nn, out_act_func)()\n",
    "\n",
    "        input_dims = [input_dim] + num_neurons\n",
    "        output_dims = num_neurons + [output_dim]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            is_last = True if i == len(input_dims) - 1 else False\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if is_last:\n",
    "                self.layers.append(self.out_act_func)\n",
    "            else:\n",
    "                self.layers.append(self.hidden_act_func)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer(xs)\n",
    "        return xs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = NaiveMultiLayerPerceptron(10, 1, [20, 12], 'ReLU', 'Identity')\n",
    "    print(net)\n",
    "\n",
    "    xs = torch.randn(size=(12, 10))\n",
    "    ys = net(xs)\n",
    "    print(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6JDCzjTHXQc"
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, max_size):\n",
    "        # deque object that we've used for 'episodic_memory' is not suitable for random sampling\n",
    "        # here, we instead use a fix-size array to implement 'buffer'\n",
    "        self.buffer = [None] * max_size\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = sample(range(self.size), batch_size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehj0EAv7CD-B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        \"\"\"\n",
    "        :param state_dim: input state dimension\n",
    "        :param action_dim: action dimension\n",
    "        :param qnet: main q network\n",
    "        :param qnet_target: target q network\n",
    "        :param lr: learning rate\n",
    "        :param gamma: discount factor of MDP\n",
    "        :param epsilon: E-greedy factor\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        # target network related\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        #prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        #if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "        if random.random() <= self.epsilon: # random\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "def prepare_training_inputs(sampled_exps, device='cpu'):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    for sampled_exp in sampled_exps:\n",
    "        states.append(sampled_exp[0])\n",
    "        actions.append(sampled_exp[1])\n",
    "        rewards.append(sampled_exp[2])\n",
    "        next_states.append(sampled_exp[3])\n",
    "        dones.append(sampled_exp[4])\n",
    "\n",
    "    states = torch.cat(states, dim=0).float().to(device)\n",
    "    actions = torch.cat(actions, dim=0).to(device)\n",
    "    rewards = torch.cat(rewards, dim=0).float().to(device)\n",
    "    next_states = torch.cat(next_states, dim=0).float().to(device)\n",
    "    dones = torch.cat(dones, dim=0).float().to(device)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlcHcXAoCUNq"
   },
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    actions = (\"do_nothing\",\n",
    "               \"harvest_minerals\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \"attack\")\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        if len(idle_scvs) > 0:\n",
    "            mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                               if unit.unit_type in [\n",
    "                                   units.Neutral.BattleStationMineralField,\n",
    "                                   units.Neutral.BattleStationMineralField750,\n",
    "                                   units.Neutral.LabMineralField,\n",
    "                                   units.Neutral.LabMineralField750,\n",
    "                                   units.Neutral.MineralField,\n",
    "                                   units.Neutral.MineralField750,\n",
    "                                   units.Neutral.PurifierMineralField,\n",
    "                                   units.Neutral.PurifierMineralField750,\n",
    "                                   units.Neutral.PurifierRichMineralField,\n",
    "                                   units.Neutral.PurifierRichMineralField750,\n",
    "                                   units.Neutral.RichMineralField,\n",
    "                                   units.Neutral.RichMineralField750\n",
    "                               ]]\n",
    "            scv = random.choice(idle_scvs)\n",
    "            distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "            mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(supply_depots) == 0 and obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            supply_depot_xy = (22, 26) if self.base_top_left else (35, 42)\n",
    "            distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                \"now\", scv.tag, supply_depot_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        if (len(completed_supply_depots) > 0 and len(barrackses) == 0 and\n",
    "                obs.observation.player.minerals >= 150 and len(scvs) > 0):\n",
    "            barracks_xy = (22, 21) if self.base_top_left else (35, 45)\n",
    "            distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                \"now\", scv.tag, barracks_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if len(marines) > 0:\n",
    "            attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            x_offset = random.randint(-4, 4)\n",
    "            y_offset = random.randint(-4, 4)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        return actions.RAW_FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6t35XWVbCc8h"
   },
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0ZqigqbCuk2"
   },
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "\n",
    "        self.s_dim = 21\n",
    "        self.a_dim = 6\n",
    "        \n",
    "        self.lr = 1e-4 * 1\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.memory_size = 200000\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_min = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.init_sampling = 4000\n",
    "        self.target_update_interval = 10\n",
    "\n",
    "        self.data_file_qnet = DATA_FILE_QNET\n",
    "        self.data_file_qnet_target = DATA_FILE_QNET_TARGET\n",
    "        self.score_file = SCORE_FILE\n",
    "        \n",
    "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        self.qnetwork_target = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        if os.path.isfile(self.data_file_qnet + '.pt'):\n",
    "            self.qnetwork.load_state_dict(torch.load(self.data_file_qnet + '.pt'))\n",
    "            \n",
    "        if os.path.isfile(self.data_file_qnet_target + '.pt'):\n",
    "            self.qnetwork_target.load_state_dict(torch.load(self.data_file_qnet_target + '.pt'))\n",
    "        \n",
    "        # initialize target network same as the main network.\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork.state_dict())\n",
    "\n",
    "        self.dqn = DQN(state_dim=self.s_dim,\n",
    "                             action_dim=self.a_dim,\n",
    "                             qnet=self.qnetwork,\n",
    "                             qnet_target=self.qnetwork_target,\n",
    "                             lr=self.lr,\n",
    "                             gamma=self.gamma,\n",
    "                             epsilon=self.epsilon).to(device)\n",
    "        \n",
    "        self.memory = ExperienceReplayMemory(self.memory_size)\n",
    "        \n",
    "        self.print_every = 1\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        self.new_game()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        \n",
    "        # epsilon scheduling\n",
    "        # slowly decaying_epsilon\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "        self.dqn.epsilon = torch.tensor(self.epsilon).to(device)\n",
    "        \n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        state = self.get_state(obs)\n",
    "        state = torch.tensor(state).float().view(1, self.s_dim).to(device)\n",
    "        action_idx = self.dqn.choose_action(state)\n",
    "        action = self.actions[action_idx]\n",
    "        done = True if obs.last() else False\n",
    "\n",
    "        if self.previous_action is not None:\n",
    "            experience = (self.previous_state.to(device),\n",
    "                          torch.tensor(self.previous_action).view(1, 1).to(device),\n",
    "                          torch.tensor(obs.reward).view(1, 1).to(device),\n",
    "                          state.to(device),\n",
    "                          torch.tensor(done).view(1, 1).to(device))\n",
    "            self.memory.push(experience)\n",
    "        \n",
    "        self.cum_reward += obs.reward\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action_idx\n",
    "        \n",
    "        if obs.last():\n",
    "            self.episode_count = self.episode_count + 1\n",
    "            \n",
    "            if len(self.memory) >= self.init_sampling:\n",
    "                # training dqn\n",
    "                sampled_exps = self.memory.sample(self.batch_size)\n",
    "                sampled_exps = prepare_training_inputs(sampled_exps, device)\n",
    "                self.dqn.learn(*sampled_exps)\n",
    "\n",
    "            if self.episode_count % self.target_update_interval == 0:\n",
    "                self.dqn.qnet_target.load_state_dict(self.dqn.qnet.state_dict())\n",
    "\n",
    "            if self.episode_count % self.print_every == 0:\n",
    "                msg = (self.episode_count, self.cum_reward, self.epsilon)\n",
    "                print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
    "            print(self.data_file_qnet)\n",
    "            torch.save(self.dqn.qnet.state_dict(), self.data_file_qnet + '.pt')\n",
    "            torch.save(self.dqn.qnet_target.state_dict(), self.data_file_qnet_target + '.pt')\n",
    "\n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(self.score_file + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            #writer.add_scalar(\"Loss/train\", self.cum_loss/obs.observation.game_loop, self.episode_count)\n",
    "            writer.add_scalar(\"Score\", self.cum_reward, self.episode_count)\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0FxrDQgTMW1"
   },
   "outputs": [],
   "source": [
    "### unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "from pysc2.lib import actions\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", \"RAW\", sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"use_raw_units\", True,\n",
    "                      \"Whether to include raw units.\")\n",
    "    flags.DEFINE_integer(\"raw_resolution\", 64, \"Raw Resolution.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgentWithRawActsAndRawObs\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "        feature_screen=FLAGS.feature_screen_size,\n",
    "        feature_minimap=FLAGS.feature_minimap_size,\n",
    "        rgb_screen=FLAGS.rgb_screen_size,\n",
    "        rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "        action_space=FLAGS.action_space,\n",
    "        use_raw_units=FLAGS.use_raw_units,\n",
    "        raw_resolution=FLAGS.raw_resolution),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    #env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgentWithRawActsAndRawObs)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      #agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(TerranRandomAgent)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwFx7U8CGC3Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0913 08:47:17.788940 139845434127744 sc_process.py:135] Launching SC2: /home/jupyter/StarCraftII/Versions/Base59877/SC2_x64 -listen 127.0.0.1 -port 18436 -dataDir /home/jupyter/StarCraftII/ -tempDir /tmp/sc-m_09hcje/\n",
      "I0913 08:47:17.834607 139845434127744 remote_controller.py:167] Connecting to: ws://127.0.0.1:18436/sc2api, attempt: 0, running: True\n",
      "I0913 08:47:18.837372 139845434127744 remote_controller.py:167] Connecting to: ws://127.0.0.1:18436/sc2api, attempt: 1, running: True\n",
      "I0913 08:47:19.839962 139845434127744 remote_controller.py:167] Connecting to: ws://127.0.0.1:18436/sc2api, attempt: 2, running: True\n",
      "I0913 08:47:25.117517 139845434127744 sc2_env.py:314] Environment is ready\n",
      "I0913 08:47:25.121968 139845434127744 sc2_env.py:507] Starting episode 1: [terran, terran] on Simple64\n",
      "I0913 08:47:49.847897 139845434127744 sc2_env.py:725] Episode 1 finished after 8704 game steps. Outcome: [1], reward: [1], score: [5035]\n",
      "I0913 08:47:49.859371 139845434127744 sc2_env.py:752] Environment Close\n",
      "I0913 08:47:49.962085 139845434127744 sc_process.py:232] Shutdown gracefully.\n",
      "I0913 08:47:49.962707 139845434127744 sc_process.py:210] Shutdown with return code: -15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    1 | Cumulative Reward :    1 | Epsilon : 1.000\n",
      "~/data_file/rlagent_with_vanilla_dqn_qnet\n",
      "Took 24.737 seconds for 1089 steps: 44.022 fps\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/data_file/rlagent_with_vanilla_dqn_qnet.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-60174cc17dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-efd1638a7e35>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   \u001b[0mrun_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-efd1638a7e35>\u001b[0m in \u001b[0;36mrun_thread\u001b[0;34m(agent_classes, players, map_name, visualize)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m#env = available_actions_printer.AvailableActionsPrinter(env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_cls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mrun_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_agent_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pysc2/env/run_loop.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(agents, env, max_frames, max_episodes)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         actions = [agent.step(timestep)\n\u001b[0;32m---> 43\u001b[0;31m                    for agent, timestep in zip(agents, timesteps)]\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_frames\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pysc2/env/run_loop.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         actions = [agent.step(timestep)\n\u001b[0;32m---> 43\u001b[0;31m                    for agent, timestep in zip(agents, timesteps)]\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_frames\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-cd2f9a97a71b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file_qnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file_qnet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_file_qnet_target\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/data_file/rlagent_with_vanilla_dqn_qnet.pt'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36uEfbuUGGwS"
   },
   "outputs": [],
   "source": [
    "!ls ~/data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training StarCraft 2 Agent under Colab",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
